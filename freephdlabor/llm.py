import json
import os
import re
import base64
from typing import List, Union, Optional, Dict, Any

import backoff
import openai

MAX_NUM_TOKENS = 4096

# Available LLMs via NewAPI (newapi.tsingyuai.com/v1)
# Only these models are supported by NewAPI
AVAILABLE_LLMS = [
    "gpt-5-nano",
    "gpt-5-mini",
    "gpt-5",
    "gpt-4o",
]


# Get N responses from a single message, used for ensembling.
@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))
def get_batch_responses_from_llm(
        msg,
        client,
        model,
        system_message,
        print_debug=False,
        msg_history=None,
        temperature=0.75,
        n_responses=1,
):
    """Get multiple responses from LLM via NewAPI for ensembling."""
    if msg_history is None:
        msg_history = []

    new_msg_history = msg_history + [{"role": "user", "content": msg}]

    # Use OpenAI SDK format for all models via NewAPI
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_message},
            *new_msg_history,
        ],
        temperature=temperature,
        max_tokens=MAX_NUM_TOKENS,
        n=n_responses,
    )
    content = [r.message.content for r in response.choices]
    new_msg_history = [
        new_msg_history + [{"role": "assistant", "content": c}] for c in content
    ]

    if print_debug:
        # Just print the first one.
        print()
        print("*" * 20 + " LLM START " + "*" * 20)
        for j, msg in enumerate(new_msg_history[0]):
            print(f'{j}, {msg["role"]}: {msg["content"]}')
        print(content)
        print("*" * 21 + " LLM END " + "*" * 21)
        print()

    return content, new_msg_history


@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))
def get_response_from_llm(
        msg,
        client,
        model,
        system_message,
        print_debug=False,
        msg_history=None,
        temperature=0.75,
):
    """Get response from LLM via NewAPI.

    All models use OpenAI SDK with NewAPI endpoint.
    Model-specific parameter handling is done by NewAPI.
    """
    if msg_history is None:
        msg_history = []

    new_msg_history = msg_history + [{"role": "user", "content": msg}]

    # Use OpenAI SDK format for all models via NewAPI
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_message},
            *new_msg_history,
        ],
        temperature=temperature,
        max_tokens=MAX_NUM_TOKENS,
        n=1,
    )
    content = response.choices[0].message.content
    new_msg_history = new_msg_history + [{"role": "assistant", "content": content}]

    if print_debug:
        print()
        print("*" * 20 + " LLM START " + "*" * 20)
        for j, msg in enumerate(new_msg_history):
            print(f'{j}, {msg["role"]}: {msg["content"]}')
        print(content)
        print("*" * 21 + " LLM END " + "*" * 21)
        print()

    return content, new_msg_history


def extract_json_between_markers(llm_output):
    # Regular expression pattern to find JSON content between ```json and ```
    for _ in range(10):
        print("!!!\n")
    print(llm_output)
    for _ in range(10):
        print("@@@\n")
    json_pattern = r"```json(.*?)```"
    matches = re.findall(json_pattern, llm_output, re.DOTALL)

    if not matches:
        # Fallback: Try to find any JSON-like content in the output
        json_pattern = r"\{.*?\}"
        matches = re.findall(json_pattern, llm_output, re.DOTALL)

    for json_string in matches:
        json_string = json_string.strip()
        try:
            parsed_json = json.loads(json_string)
            return parsed_json
        except json.JSONDecodeError:
            # Attempt to fix common JSON issues
            try:
                # Remove invalid control characters
                json_string_clean = re.sub(r"[\x00-\x1F\x7F]", "", json_string)
                parsed_json = json.loads(json_string_clean)
                return parsed_json
            except json.JSONDecodeError:
                continue  # Try next match
    return None  # No valid JSON found

def extract_json_between_markers(text: str) -> Optional[Dict[str, Any]]:
    """
    Extract JSON from text between ```json and ``` markers.
    
    Args:
        text: Text containing JSON
        
    Returns:
        Extracted JSON as a dictionary or None if extraction fails
    """
    # Try to find JSON between ```json and ``` markers
    matches = re.findall(r"```json\s*([\s\S]*?)\s*```", text)
    if matches:
        try:
            return json.loads(matches[0])
        except json.JSONDecodeError:
            return None
    
    # Try to find JSON between ``` and ``` markers without json specifier
    matches = re.findall(r"```\s*([\s\S]*?)\s*```", text)
    if matches:
        try:
            return json.loads(matches[0])
        except json.JSONDecodeError:
            return None
    
    return None

def encode_image_to_base64(image_data: Union[str, bytes, List[bytes]]) -> str:
    """
    Encode image data to base64 string for VLM usage.
    
    Args:
        image_data: Can be file path (str), raw bytes, or list of bytes
        
    Returns:
        Base64 encoded string
    """
    if isinstance(image_data, str):
        # File path
        with open(image_data, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")
    elif isinstance(image_data, list):
        # List of bytes (take first element)
        return base64.b64encode(image_data[0]).decode("utf-8")
    elif isinstance(image_data, bytes):
        # Raw bytes
        return base64.b64encode(image_data).decode("utf-8")
    else:
        raise TypeError(f"Unsupported image data type: {type(image_data)}")


@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))
def get_response_from_vlm(
    prompt: str,
    images: List[str],
    client,
    model: str,
    system_message: str = "",
    print_debug: bool = False,
    msg_history: Optional[List[Dict]] = None,
    temperature: float = 0.75,
) -> tuple[str, List[Dict]]:
    """
    Get response from Vision-Language Model with image inputs via NewAPI.

    Args:
        prompt: Text prompt for the VLM
        images: List of image file paths
        client: OpenAI client instance (configured for NewAPI)
        model: Model name (e.g., gpt-4o)
        system_message: System message for the conversation
        print_debug: Whether to print debug information
        msg_history: Previous conversation history
        temperature: Sampling temperature

    Returns:
        Tuple of (response_content, updated_message_history)
    """
    if msg_history is None:
        msg_history = []

    # Prepare message content with text and images
    content = [{"type": "text", "text": prompt}]

    # Add images to content
    for image_path in images:
        try:
            base64_image = encode_image_to_base64(image_path)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            })
        except Exception as e:
            print(f"Warning: Failed to encode image {image_path}: {e}")
            continue

    # Build message history
    new_msg_history = msg_history + [{"role": "user", "content": content}]

    # Prepare messages for API call
    messages = []
    if system_message:
        messages.append({"role": "system", "content": system_message})
    messages.extend(new_msg_history)

    # Make API call via NewAPI (all models use OpenAI SDK format)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=MAX_NUM_TOKENS,
        n=1,
    )
    content_response = response.choices[0].message.content

    # Update message history
    new_msg_history = new_msg_history + [{"role": "assistant", "content": content_response}]

    if print_debug:
        print()
        print("*" * 20 + " VLM START " + "*" * 20)
        for j, msg in enumerate(new_msg_history):
            print(f'{j}, {msg["role"]}: {msg["content"]}')
        print(content_response)
        print("*" * 21 + " VLM END " + "*" * 21)
        print()

    return content_response, new_msg_history


def create_vlm_client(model: str = "gpt-4o"):
    """
    Create a VLM client for vision tasks via NewAPI.

    Args:
        model: VLM model name (defaults to gpt-4o)

    Returns:
        Tuple of (client, model_name)
    """
    api_key = os.environ.get("OPENAI_API_KEY", "")
    api_base = os.environ.get("OPENAI_BASE_URL", "https://newapi.tsingyuai.com/v1")

    # Use gpt-4o as default VLM model via NewAPI
    if "gpt" not in model:
        model = "gpt-4o"

    print(f"Using NewAPI VLM ({api_base}) with model {model}.")
    return openai.OpenAI(api_key=api_key, base_url=api_base), model


def create_client(model):
    """Create an OpenAI client configured for NewAPI.

    All models are accessed via NewAPI (newapi.tsingyuai.com/v1) using OpenAI SDK.
    Model fallback and retry are handled by NewAPI, not in code.
    """
    api_key = os.environ.get("OPENAI_API_KEY", "")
    api_base = os.environ.get("OPENAI_BASE_URL", "https://newapi.tsingyuai.com/v1")

    # All models use NewAPI with OpenAI SDK
    print(f"Using NewAPI ({api_base}) with model {model}.")
    return openai.OpenAI(api_key=api_key, base_url=api_base), model
